---
title: "10: Regression Extensions"
subtitle: "Statistical Research Methods"
author: "<large>J. Seawright</large>"
institute: "<small>Northwestern Political Science</small>" 
date: "April 30, 2025"
output: 
  xaringan::moon_reader:
    css: xaringan-themer.css
  
---
class: center, middle

```{css, echo=FALSE}
pre {
  max-height: 400px;
  overflow-y: auto;
}

pre[class] {
  max-height: 200px;
}
```

```{css, echo=FALSE}
.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
```

```{r, load_refs, include=FALSE, cache=FALSE}
# Initializes the bibliography
library(RefManageR)

library(ggplot2)
library(dplyr)
library(readr)
library(nlme)
library(jtools)
library(hrbrthemes)
library(mice)
library(haven)
library(GGally)
library(car)
options(warn=-1)

BibOptions(check.entries = FALSE,
           bib.style = "authoryear", # Bibliography style
           max.names = 3, # Max author names displayed in bibliography
           sorting = "nyt", #Name, year, title sorting
           cite.style = "authoryear", # citation style
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
#myBib <- ReadBib("assets/myBib.bib", check = FALSE)
# Note: don't forget to clear the knitr cache to account for changes in the
# bibliography.
```
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer,MnSymbol)
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono"),
  text_font_size = "1.3rem"
)
```

---

In addition to assuming that we have the right control variables for causal inference, a good regression requires that we get some statistical assumptions right.

---

- Is the error term *independent* across cases, and of the causal variable?

- Are the relationships of interest represented using the right *mathematical form*?

- Do any of the data points have too much *influence* on the results?

- Are the right-hand-side variables *sufficiently uncorrelated* with each other?

---

- Does the error term have about the *same variance* across all the cases?

- Is the error term about *normally distributed*?

- Does the main causal variable have a *constant effect* across all cases?

```{r, echo = TRUE, out.width="100%", fig.retina = 1, fig.align='center'}
uscinski2 <- read_dta("uscinski2.dta")
uscinski2$conspiratorial <- with(uscinski2, con1 + con2 + con3 +con4)
qft3.lm <- lm(qanonft~conspiratorial+pid+ideo+female+age+income+edu, data = uscinski2)
peruemotions <- read.csv("peruemotions.csv")
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
library(ggfortify)
autoplot(qft3.lm)
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
dfbetasPlots(qft3.lm)
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
hist(uscinski2$qanonft)
```

---

When a variable is highly *skewed*, i.e., it is concentrated on one side of the distribution or the other, it can't be normally distributed, and it makes several of the other assumptions harder to meet.

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
uscinski2$logqft <- log(uscinski2$qanonft + 1)
hist(uscinski2$logqft)
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
qft3.5.lm <- lm(logqft~conspiratorial+pid+ideo+female+age+income+edu, data = uscinski2)
summary(qft3.5.lm)
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
autoplot(qft3.5.lm)
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
ggcoef(qft3.5.lm)
```

---

Let's check if our variables are sufficiently uncorrelated with each other!

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
library(car)
vif(qft3.lm)
```

---

Let's check if our error term has approximately the same variance across all the cases!

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
ncvTest(qft3.5.lm)
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
library(lmtest)
bptest(qft3.5.lm)
```

---

Let's check if we've represented the relationships using the right mathematical form!

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
reset(qft3.5.lm, power = 2:3, type = "regressor")
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
qft3.75.lm <- lm(logqft~conspiratorial+pid+ideo+female+age+I(age^2)+income+I(income^2)+edu+I(edu^2), data = uscinski2)
summary(qft3.75.lm)
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
reset(qft3.75.lm, power = 2:3, type = "regressor")
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
qft3.9.lm <- lm(logqft~conspiratorial+pid+ideo+I(ideo^2)+female+age+I(age^2)+income+I(income^2)+edu+I(edu^2)+I(edu^3), data = uscinski2)
summary(qft3.9.lm)
```

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
reset(qft3.9.lm, power = 2:3, type = "regressor")

vif(qft3.9.lm)
```

---

Let's check if the error term is approximately normal!

---
```{r, echo = TRUE, out.width="70%", fig.retina = 1, fig.align='center'}
library(tseries)
jarque.bera.test(rstandard(qft3.5.lm))
```


---
### Linear Probability Models

We can interpret the model as predicting the probability that the outcome equals 1. Then everything else mostly works okay, with some weirdness.

---
### GLMs

Or we can avoid the weirdness by wrapping our regression in a nonlinear transformation, to force the model to always produce valid probabilities.

---
### Three Steps for Zero-One Outcomes

-   Think of the regression as predicting the probability of a one,
    rather than the mean.

-   Use a nonlinear function to stretch the zero-one interval to the
    whole number line.

-   Find a way to estimate this nonlinear model.

---
```{r, echo = TRUE, out.width="80%", fig.retina = 1}
example1.logit <- glm(outsidervote ~ simpletreat, data=peruemotions, family=binomial(link=logit))
summary(example1.logit)
```

---
```{r, echo = TRUE, out.width="80%", fig.retina = 1}
logitplot <- peruemotions %>% ggplot(aes(x=simpletreat, y=outsidervote)) + geom_point() +
      stat_smooth(method="glm", color="green", se=FALSE, fullrange=TRUE,
                method.args = list(family=binomial)) + xlim(-10,10)
```

---
```{r, echo = TRUE, out.width="60%", fig.retina = 1}
logitplot
```

---
### Interpreting Coefficients in Zero-One Models

-   In linear regression, $\beta$ tells you how much the mean response
    changes for a one-unit change in the relevant independent variable.

-   That's not usually true for a GLM.

---
### Interpreting Logit Coefficients

-   As a very rough approximation, you can divide logit coefficients by 4 to get a sense of their magnitude.

---
### Interpreting Logit Coefficients

-   For actual analysis, or for use in a paper, you should probably calculate the average marginal effect, i.e., the average effect of moving every case in the data by one unit on the explanatory variable.

---
```{r, echo = TRUE, out.width="80%", fig.retina = 1}
library(margins)
summary(margins(example1.logit))
```

---
```{r, echo = TRUE, out.width="80%", fig.retina = 1}
plot(margins(example1.logit))
```
